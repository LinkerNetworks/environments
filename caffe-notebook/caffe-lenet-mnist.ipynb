{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pylab import *\n",
    "import os, struct\n",
    "import gzip\n",
    "import shutil\n",
    "import caffe\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import lmdb\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "from caffe import layers as L, params as P\n",
    "from caffe.proto import caffe_pb2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lenet(lmdb, batch_size):\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "def download_dataset(fn):\n",
    "    dataset_gzfilename = os.path.join(dataset_dir, '%s.gz' % fn)\n",
    "    urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/%s.gz' % fn, dataset_gzfilename)\n",
    "    with gzip.open(dataset_gzfilename, 'rb') as f_in, open(dataset_gzfilename[:-3], 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print('download dataset: %s' % fn)\n",
    "    \n",
    "def check_dir(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        print('make dir: %s' % dir_path)\n",
    "        os.mkdir(dir_path)\n",
    "        \n",
    "def load_mnist(fname_img, fname_lbl, digits=np.arange(10)):\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def convert_to_lmdb(images, labels, lmdb_path):\n",
    "    N =images.shape[0]\n",
    "    hight=images.shape[1]\n",
    "    width=images.shape[2]\n",
    "    X=images.reshape(N, 1, hight, width)\n",
    "    y = labels\n",
    "    map_size = X.nbytes * 10\n",
    "    env = lmdb.open(lmdb_path, map_size=map_size)\n",
    "    with env.begin(write=True) as txn:\n",
    "        for i in range(N):\n",
    "            datum = caffe.proto.caffe_pb2.Datum()\n",
    "            datum.channels = X.shape[1]\n",
    "            datum.height = X.shape[2]\n",
    "            datum.width = X.shape[3]\n",
    "            datum.data = X[i].tostring()\n",
    "            datum.label = int(y[i])\n",
    "            str_id = '{:08}'.format(i)\n",
    "            txn.put(str_id.encode('ascii'), datum.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workdir_prefix = '/batch'\n",
    "check_dir(workdir_prefix)\n",
    "dataset_dir = os.path.join(workdir_prefix, 'data')\n",
    "check_dir(dataset_dir)\n",
    "model_dir = os.path.join(workdir_prefix, 'models')\n",
    "check_dir(model_dir)\n",
    "snapshot_dir = os.path.join(model_dir, 'lenet')\n",
    "check_dir(snapshot_dir)\n",
    "mnist_dataset_filename = ['train-images-idx3-ubyte', 'train-labels-idx1-ubyte', 't10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte']\n",
    "train_net_path = os.path.join(model_dir, 'lenet_auto_train.prototxt')\n",
    "test_net_path = os.path.join(model_dir, 'lenet_auto_test.prototxt')\n",
    "solver_config_path = os.path.join(model_dir, 'lenet_auto_solver.prototxt')\n",
    "mnist_train_lmdb_path = os.path.join(model_dir, 'mnist_train_lmdb')\n",
    "check_dir(mnist_train_lmdb_path)\n",
    "mnist_test_lmdb_path = os.path.join(model_dir, 'mnist_test_lmdb')\n",
    "check_dir(mnist_test_lmdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in mnist_dataset_filename:\n",
    "    if not os.path.isfile(os.path.join(dataset_dir, fn)):\n",
    "        download_dataset(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('convert dataset to lmdb format.')\n",
    "train_mnist_image, train_mnist_label = load_mnist(os.path.join(dataset_dir, mnist_dataset_filename[0]), os.path.join(dataset_dir, mnist_dataset_filename[1]), np.arange(10))\n",
    "convert_to_lmdb(train_mnist_image, train_mnist_label, mnist_train_lmdb_path)\n",
    "test_mnist_image, test_mnist_label = load_mnist(os.path.join(dataset_dir, mnist_dataset_filename[2]), os.path.join(dataset_dir, mnist_dataset_filename[3]), np.arange(10))\n",
    "convert_to_lmdb(test_mnist_image, test_mnist_label, mnist_test_lmdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(lenet(mnist_train_lmdb_path, 64)))    \n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(lenet(mnist_test_lmdb_path, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = caffe_pb2.SolverParameter()\n",
    "s.random_seed = 0xCAFFE\n",
    "s.train_net = train_net_path\n",
    "s.test_net.append(test_net_path)\n",
    "s.test_interval = 500\n",
    "s.test_iter.append(100)\n",
    "s.max_iter = 10000\n",
    "s.type = \"SGD\"\n",
    "s.base_lr = 0.01\n",
    "s.momentum = 0.9\n",
    "s.weight_decay = 5e-4\n",
    "s.lr_policy = 'inv'\n",
    "s.gamma = 0.0001\n",
    "s.power = 0.75\n",
    "s.display = 1000\n",
    "s.snapshot = 5000\n",
    "s.snapshot_prefix = snapshot_dir\n",
    "s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "with open(solver_config_path, 'w') as f:\n",
    "    f.write(str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 250\n",
    "train_loss = zeros(niter)\n",
    "test_acc = zeros(niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for it in range(niter):\n",
    "    solver.step(1)\n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    correct = 0\n",
    "    for test_it in range(100):\n",
    "        solver.test_nets[0].forward()\n",
    "        correct += sum(solver.test_nets[0].blobs['score'].data.argmax(1)\n",
    "                       == solver.test_nets[0].blobs['label'].data)\n",
    "    test_acc[it] = correct / 1e4\n",
    "    print('[epoch %d] loss: %f, accuracy: %f' % (it + 1, train_loss[it], test_acc[it]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax1 = subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(arange(niter), train_loss)\n",
    "ax2.plot(arange(niter), test_acc, 'r')\n",
    "ax1.set_xlabel('iteration')\n",
    "ax1.set_ylabel('train loss')\n",
    "ax2.set_ylabel('test accuracy')\n",
    "ax2.set_title('Custom Test Accuracy: {:.2f}'.format(test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
